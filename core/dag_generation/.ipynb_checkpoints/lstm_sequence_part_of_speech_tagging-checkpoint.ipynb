{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0e3002ee30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdB': 0, 'BC': 1, 'CB': 2, 'CE': 3, 'AfD': 4, 'DB': 5, 'BB': 6, 'AfB': 7, 'BE': 8, 'AfC': 9, 'CC': 10, 'CD': 11, 'BD': 12, 'DE': 13, 'AfE': 14, 'AdC': 15, 'DC': 16}\n",
      "{'1-2': 0, '2': 1, '2-3-3-1-4': 2, '2-4': 3, '2-5': 4, '': 5, '2-3-3': 6, '1': 7, '2-3-3-5': 8, '1-4': 9, '2-3-1-4': 10, '2-3': 11, '3-5': 12, '2-3-3-1-5': 13, '2-3-5': 14, '1-2-3-2-2': 15, '2-3-4-7-2': 16, '2-6-3': 17, '2-3-1': 18, '2-6-3-5': 19, '2-3-4': 20, '2-6-3-4-7-2-5': 21, '4': 22, '1-2-3-1-2': 23, '1-3-2': 24, '2-3-2': 25, '1-2-3-5': 26, '2-3-4-7-1-7-1-2-4': 27, '2-3-4-7-4-7-1-2': 28, '2-3-4-7-2-4': 29, '2-4-7-1-5': 30, '2-3-4-7-1-2-5': 31, '2-3-3-3-2': 32, '2-3-1-2-4': 33, '2-2': 34, '3-2': 35, '2-6-3-4-7-1': 36, '2-3-4-7-1-2': 37, '2-6-3-4-7-2': 38, '3-3-4': 39, '6-3-5': 40, '1-2-5': 41, '6-3-4-7-2-5': 42, '5': 43, '2-3-4-7-2-4-2': 44}\n",
      "{0: '1-2', 1: '2', 2: '2-3-3-1-4', 3: '2-4', 4: '2-5', 5: '', 6: '2-3-3', 7: '1', 8: '2-3-3-5', 9: '1-4', 10: '2-3-1-4', 11: '2-3', 12: '3-5', 13: '2-3-3-1-5', 14: '2-3-5', 15: '1-2-3-2-2', 16: '2-3-4-7-2', 17: '2-6-3', 18: '2-3-1', 19: '2-6-3-5', 20: '2-3-4', 21: '2-6-3-4-7-2-5', 22: '4', 23: '1-2-3-1-2', 24: '1-3-2', 25: '2-3-2', 26: '1-2-3-5', 27: '2-3-4-7-1-7-1-2-4', 28: '2-3-4-7-4-7-1-2', 29: '2-3-4-7-2-4', 30: '2-4-7-1-5', 31: '2-3-4-7-1-2-5', 32: '2-3-3-3-2', 33: '2-3-1-2-4', 34: '2-2', 35: '3-2', 36: '2-6-3-4-7-1', 37: '2-3-4-7-1-2', 38: '2-6-3-4-7-2', 39: '3-3-4', 40: '6-3-5', 41: '1-2-5', 42: '6-3-4-7-2-5', 43: '5', 44: '2-3-4-7-2-4-2'}\n"
     ]
    }
   ],
   "source": [
    "# lele construct the training datasets\n",
    "\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "# definition\n",
    "id_to_nodes = {\"Af\":\"FileScan FactTable\", \"Ad\":\"FileScan DimTable\", \"B\":\"BroadcastHashJoin\", \"C\":\"SortMergeJoin\", \"D\":\"Union\"}\n",
    "id_to_operators = {\"\":\"\", \"1\":\"Filter\", \"2\":\"Project\", \"3\":\"HashAggregate-HashAggregate\", \"4\":\"Sort\", \"5\":\"TakeOrderedAndProject\",\"6\":\"Expand\",\"7\":\"Window\"}\n",
    "\n",
    "training_data = [\n",
    "    (\"AdB BC CB BC CE\".split(), [\"1-2\", \"2\", \"2-3-3-1-4\", \"2-4\",\"2-5\"]), #TPC-DS Q1\n",
    "    (\"AfD DB BB BC CE\".split(), [\"1-2\", \"\", \"2-3-3\", \"2-4\",\"2-4\"]), #TPC-DS Q2\n",
    "    (\"AfB BB BE\".split(), [\"1\", \"2\", \"2-3-3-5\"]), # TPC-DS Q3\n",
    "    (\"AfC CB BC CC CC CC CC CE\".split(), [\"1-4\",\"2\",\"2-3-1-4\",\"\",\"2\",\"2\",\"2\",\"2-5\"]), # TPC-DS Q4\n",
    "    (\"AfC CD DB BB BD DE\".split(), [\"1-4\",\"2\",\"\",\"2\",\"2-3\",\"3-5\"]), # TPC-DS Q5\n",
    "    (\"AfB BC CB BC CE\".split(), [\"1\", \"2-4\", \"2\", \"2-4\", \"2-3-3-1-5\"]), #TPC-DS Q6\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q7\n",
    "    (\"AfC CB BE\".split(), [\"1-4\",\"2-3\",\"2-3-5\"]), #TPC-DS Q8\n",
    "    (\"AfE\".split(), [\"1-2-3-2-2\"]), #TPC-DS Q9\n",
    "    (\"AdB BC CC CB BB BE\".split(), [\"1-2\",\"2-4\",\"\",\"1-2\",\"2\",\"2-3-5\"]), #TPC-DS Q10\n",
    "    (\"AfC CB BC CC CC CE\".split(), [\"1-4\",\"2\",\"2-3-1-4\",\"\",\"2\",\"2\"]), #TPC-DS Q11\n",
    "    (\"AfB BB BE\".split(), [\"1\",\"2\",\"2-3-4-7-2\"]), #TPC-DS Q12\n",
    "    (\"AfB BB BB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q13\n",
    "    #TPC-DS Q14a\n",
    "    #TPC-DS Q14b\n",
    "    (\"AfC CC CB BE\".split(), [\"1-4\",\"2-4\",\"2\",\"2-3\"]), #TPC-DS Q15\n",
    "    (\"AfC CC CB BB BB BE\".split(), [\"1-4\",\"2\",\"\",\"2\",\"2\",\"2-3-3\"]), #TPC-DS Q16\n",
    "    (\"AfC CC CB BB BB BB BE\".split(), [\"1-4\",\"2-4\",\"2\",\"2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q17\n",
    "    (\"AfB BC CB BC CB BB BE\".split(), [\"1-2\",\"2-4\",\"2\",\"2-4\",\"2\",\"2\",\"2-6-3\"]), #TPC-DS Q18\n",
    "    (\"AdB BB BC CB BB BE\".split(), [\"1-2\",\"2\",\"2-4\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q19\n",
    "    (\"AfB BB BE\".split(), [\"1\",\"2\",\"2-3-4-7-2\"]), #TPC-DS Q20\n",
    "    (\"AfB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2-3-1\"]), #TPC-DS Q21\n",
    "    (\"AdB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-6-3-5\"]), #TPC-DS Q22\n",
    "    #TPC-DS Q23a\n",
    "    #TPC-DS Q23b\n",
    "    #TPC-DS Q24a\n",
    "    #TPC-DS Q24b\n",
    "    (\"AfC CC CB BB BB BB BB BE\".split(), [\"1-4\",\"2-4\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q25\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q26\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-6-3-5\"]), #TPC-DS Q27\n",
    "    #TPC-DS Q28 æœ‰subquery\n",
    "    (\"AfC CC CB BB BB BB BB BE\".split(), [\"1-4\",\"2-4\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q29\n",
    "    (\"AdB BB BC CC CB BE\".split(), [\"1-2\",\"2\",\"2-3-3-1-4\",\"2-4\",\"2\",\"2-5\"]), #TPC-DS Q30\n",
    "    (\"AdB BB BC CC CC CC CE\".split(), [\"1\",\"2\",\"2-3-4\",\"2\",\"\",\"2\",\"2-4\"]), #TPC-DS Q31\n",
    "    (\"AdB BC CB BE\".split(), [\"1-2\",\"2-3-1-4\",\"2\",\"2-3\"]), #TPC-DS Q32\n",
    "    (\"AdB BB BB BD DE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\",\"3-5\"]), #TPC-DS Q33\n",
    "    (\"AdB BB BB BC CE\".split(), [\"1-2\",\"2\",\"2\",\"2-3-1-4\",\"2-4\"]), #TPC-DS Q34\n",
    "    (\"AdB BC CC CC CB BB BE\".split(), [\"1-2\",\"2-4\",\"\",\"\",\"1-2\",\"2\",\"2-3-5\"]), #TPC-DS Q35\n",
    "    #---------33-----------\n",
    "    (\"AdB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-6-3-4-7-2-5\"]), #TPC-DS Q36\n",
    "    \n",
    "    (\"AdB BB BB BC CE\".split(), [\"1-2\",\"2\",\"2\",\"4\",\"2-3-5\"]), #TPC-DS Q37\n",
    "    (\"AdB BC CC CC CE\".split(), [\"1-2\",\"2-4\",\"2-3-4\",\"\",\"2-3\"]), #TPC-DS Q38\n",
    "    #TPC-DS Q39a\n",
    "    #TPC-DS Q39b\n",
    "    (\"AfC CB BB BB BE\".split(), [\"1-4\",\"2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q40\n",
    "\n",
    "    (\"AfB BE\".split(), [\"1-2-3-1-2\",\"1-3-2\"]), #TPC-DS Q41\n",
    "    (\"AdB BB BE\".split(), [\"1-2\",\"2\",\"2-3-2\"]), #TPC-DS Q42\n",
    "    (\"AdB BB BE\".split(), [\"1-2\",\"2\",\"2-3-2\"]), #TPC-DS Q43\n",
    "    #TPC-DS Q44\n",
    "    (\"AfB BB BB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2\",\"1-2-3-5\"]), #TPC-DS Q45\n",
    "    (\"AdB BB BB BB BC CB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-3-4\",\"2\",\"2\"]), #TPC-DS Q46\n",
    "    (\"AdB BB BB BC CC CE\".split(), [\"1\",\"2\",\"2\",\"2-3-4-7-1-7-1-2-4\",\"2\",\"2-5\"]), #TPC-DS Q47\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q48\n",
    "    (\"AfC CB BD DE\".split(), [\"1-4\",\"2\",\"2-3-4-7-4-7-1-2\",\"3-5\"]), #TPC-DS Q49\n",
    "    (\"AfC CB BB BB BE\".split(), [\"1-4\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q50\n",
    "\n",
    "    (\"AdB BC CE\".split(), [\"1-2\",\"2-3-4-7-2-4\",\"2-4-7-1-5\"]), #TPC-DS Q51\n",
    "    (\"AdB BB BE\".split(), [\"1-2\",\"2\",\"2-3-5\"]), #TPC-DS Q52\n",
    "    (\"AfB BB BE\".split(), [\"1-2\",\"2\",\"2-3-4-7-1-2-5\"]), #TPC-DS Q53\n",
    "    (\"AfD DB BB BC CC CB BB BE\".split(), [\"1-2\",\"\",\"2\",\"2-4\",\"2-4\",\"2\",\"2\",\"2-3-3-3-2\"]),#TPC-DS Q54\n",
    "    (\"AdB BB BE\".split(), [\"1-2\",\"2\",\"2-3-5\"]), #TPC-DS Q55\n",
    "    (\"AdB BB BB BD DE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\",\"3-5\"]), #TPC-DS Q56\n",
    "    (\"AfB BB BB BC CC CE\".split(), [\"1\",\"2\",\"2\",\"2-3-4-7-1-7-1-2-4\",\"2\",\"2-5\"]), #TPC-DS Q57\n",
    "    #TPC-DS Q58\n",
    "    (\"AdB BB BB BC CE\".split(), [\"1\",\"2-3\",\"2\",\"2-4\",\"2-5\"]), #TPC-DS Q59\n",
    "    (\"AdB BB BB BD DE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\",\"3-5\"]), #TPC-DS Q60\n",
    "    \n",
    "    # window\n",
    "    # id_to_nodes = {\"Af\":\"FileScan FactTable\", \"Ad\":\"FileScan DimTable\", \"B\":\"BroadcastHashJoin\", \n",
    "    # \"C\":\"SortMergeJoin\", \"D\":\"Union\"}\n",
    "    # id_to_operators = {\"\":\"\", \"1\":\"Filter\", \"2\":\"Project\", \"3\":\"HashAggregate\", \n",
    "    # \"4\":\"Sort\", \"5\":\"TakeOrderedAndProject\",\"6\":\"Expand\",\"7\":\"Window\"}\n",
    "    (\"AfB BB BB BB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-3\",\"2\"]), #TPC-DS Q61\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q62\n",
    "    (\"AfB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-3-4-7-1-2-5\"]), #TPC-DS Q63\n",
    "    (\"AfC CC CB BB BC CB BB BB BB BB BB BB BC CC CB BB BB BC CE\".split(), [\"1-4\",\"2-3-1-2-4\",\"2\",\"2\",\"2-4\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-4\",\"2-4\",\"2\",\"2\",\"2\",\"2-3-4\",\"2-4\"]), #TPC-DS Q64\n",
    "    (\"AdB BB BB BC CE\".split(), [\"1-2\",\"2-3-1\",\"2\",\"2-4\",\"2-2\"]), #TPC-DS Q65\n",
    "    (\"AdB BB BB BB BD DE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2-3\",\"3-2\"]), #TPC-DS Q66\n",
    "    (\"AdB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-6-3-4-7-1\"]), #TPC-DS Q67\n",
    "    (\"AdB BB BB BB BC CB BE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-3-4\",\"2\",\"2-5\"]), #TPC-DS Q68\n",
    "    (\"AdB BC CC CC CB BB BE\".split(), [\"1-2\",\"2-4\",\"\",\"\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q69\n",
    "    (\"AdB BB BC CB BE\".split(), [\"1\",\"2\",\"2-3-4-7-1-2\",\"\",\"2-6-3-4-7-2\"]), #TPC-DS Q70\n",
    "\n",
    "    (\"AdB BD DB BB BE\".split(), [\"1-2\",\"2\",\"\",\"2\",\"2-3-4\"]), #TPC-DS Q71\n",
    "    (\"AdC CB BB BB BB BB BB BB BB BB BC CE\".split(), [\"1-4\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-4\",\"2-3-5\"]), #TPC-DS Q72\n",
    "    (\"AdB BB BB BC CE\".split(), [\"1-2\",\"2\",\"2\",\"2-3-1-4\",\"2-4\"]), #TPC-DS Q73\n",
    "    (\"AfC CB BC CC CE\".split(), [\"1-4\",\"2\",\"2-3-1-4\",\"2\",\"2-5\"]), #TPC-DS Q74\n",
    "    (\"AfB BB BC CD DC CE\".split(), [\"1-2\",\"2\",\"2-4\",\"2\",\"3-3-4\",\"2-5\"]), #TPC-DS Q75\n",
    "    (\"AfB BB BD DE\".split(), [\"1\",\"2\",\"2\",\"3-5\"]), #TPC-DS Q76\n",
    "    (\"AdB BB BC CD DE\".split(), [\"1-2\",\"2\",\"2-3-4\",\"2\",\"6-3-5\"]), #TPC-DS Q77\n",
    "    (\"AdC CB BC CC CE\".split(), [\"1-4\",\"1-2\",\"2-3-4\",\"2\",\"1-2-5\"]), #TPC-DS Q78\n",
    "    (\"AdB BB BB BC CE\".split(), [\"1-2\",\"2\",\"2\",\"2-3-4\",\"2-5\"]), #TPC-DS Q79\n",
    "    (\"AfC CB BB BB BD DE\".split(), [\"1-4\",\"2\",\"2\",\"2\",\"2-3\",\"6-3-5\"]), #TPC-DS Q80\n",
    "\n",
    "    (\"AdB BB BC CC CC CE\".split(), [\"1-2\",\"2\",\"2-3-3-1-4\",\"2-4\",\"2-4\",\"2-5\"]), #TPC-DS Q81\n",
    "    (\"AdB BB BC CE\".split(), [\"1-2\",\"2\",\"2-4\",\"2-3-5\"]), #TPC-DS Q82\n",
    "    (\"AdB BB BB BC CC CE\".split(), [\"1-2\",\"2\",\"2\",\"2-3-4\",\"2\",\"2-5\"]), #TPC-DS Q83\n",
    "    (\"AfB BB BB BB BC CE\".split(), [\"1-2\",\"2\",\"2\",\"2\",\"2-4\",\"2-5\"]), #TPC-DS Q84\n",
    "    (\"AfC CB BB BB BB BB BB BE\".split(), [\"1-4\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q85\n",
    "    (\"AdB BB BE\".split(), [\"1-2\",\"2\",\"6-3-4-7-2-5\"]), #TPC-DS Q86\n",
    "    (\"AdB BC CC CC CE\".split(), [\"1-2\",\"2-4\",\"2-3\",\"2\",\"2-3\"]), #TPC-DS Q87\n",
    "    (\"AfB BB BB BB BB BB BB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]), #TPC-DS Q88\n",
    "    (\"AfB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2-3-4-7-1-2-5\"]), #TPC-DS Q89\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\",\"5\"]), #TPC-DS Q90\n",
    "\n",
    "    (\"AfB BB BC CB BB BB BE\".split(), [\"1\",\"2\",\"2-4\",\"2\",\"2\",\"2\",\"2-3-4\"]), #TPC-DS Q91\n",
    "    (\"AdB BC CB BE\".split(), [\"1-2\",\"2-3-1-4\",\"2\",\"2-3\"]), #TPC-DS Q92\n",
    "    (\"AfC CB BE\".split(), [\"1-4\",\"2\",\"2-3-5\"]), #TPC-DS Q93\n",
    "    (\"AfC CC CB BB BB BE\".split(), [\"1-4\",\"2\",\"\",\"2\",\"2\",\"2-3-3\"]), #TPC-DS Q94\n",
    "    (\"AfC CC CC CB BB BB BE\".split(), [\"1-4\",\"2\",\"2\",\"\",\"2\",\"2\",\"2-3-3\"]), #TPC-DS Q95\n",
    "    (\"AfB BB BB BE\".split(), [\"1-2\",\"2\",\"2\",\"2-3\"]), #TPC-DS Q96\n",
    "    (\"AdB BC CE\".split(), [\"1-2\",\"2-3-4\",\"2-3\"]), #TPC-DS Q97\n",
    "    (\"AfB BB BE\".split(), [\"1\",\"2\",\"2-3-4-7-2-4-2\"]), #TPC-DS Q98\n",
    "    (\"AfB BB BB BB BE\".split(), [\"1\",\"2\",\"2\",\"2\",\"2-3-5\"]), #TPC-DS Q99\n",
    "\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "\n",
    "tag_to_ix={}\n",
    "for sent, tags in training_data:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:  # tag has not been assigned an index yet\n",
    "            tag_to_ix[tag] = len(tag_to_ix)  # Assign each tag with a unique index\n",
    "print(tag_to_ix)\n",
    "\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "print(ix_to_tag)\n",
    "\n",
    "\n",
    "# tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7320, -3.6492, -3.7219, -4.0563, -3.8823, -4.0388, -3.5302, -3.4287,\n",
      "         -3.8623, -4.0610, -3.5525, -3.6482, -3.6492, -4.2257, -3.7142, -3.9485,\n",
      "         -4.2275, -4.1176, -3.6804, -3.7345, -4.2622, -3.6963, -3.5584, -3.6006,\n",
      "         -3.7479, -3.9142, -3.5864, -3.6503, -3.7074, -4.2238, -3.9455, -3.6400,\n",
      "         -3.4157, -3.4761, -3.9967, -3.9836, -4.1222, -4.1565, -3.5653, -4.1648,\n",
      "         -4.0081, -3.7546, -4.1730, -3.7456, -4.0374],\n",
      "        [-3.8441, -3.6434, -3.7051, -4.1040, -3.9108, -4.0057, -3.5965, -3.3642,\n",
      "         -3.8112, -4.0143, -3.5047, -3.5967, -3.6941, -4.3172, -3.7720, -3.8785,\n",
      "         -4.2472, -4.1166, -3.6361, -3.7494, -4.2783, -3.7102, -3.5623, -3.6072,\n",
      "         -3.8764, -3.8187, -3.5741, -3.7247, -3.7289, -4.3166, -3.9021, -3.6545,\n",
      "         -3.3908, -3.4072, -3.9376, -4.0259, -4.0959, -4.1847, -3.5792, -4.1263,\n",
      "         -4.0069, -3.6735, -4.3015, -3.7273, -4.0354],\n",
      "        [-3.9263, -3.6228, -3.7264, -4.0511, -3.9226, -4.0178, -3.5856, -3.5186,\n",
      "         -3.8827, -4.0407, -3.6346, -3.6318, -3.7130, -4.3293, -3.7335, -3.7889,\n",
      "         -4.2639, -4.2568, -3.6892, -3.8078, -4.2764, -3.7542, -3.4002, -3.6002,\n",
      "         -3.7294, -3.7526, -3.5209, -3.6867, -3.6466, -4.2649, -3.9689, -3.5388,\n",
      "         -3.3459, -3.3670, -3.9995, -4.1347, -4.1244, -4.0803, -3.5983, -4.1741,\n",
      "         -3.9281, -3.6784, -4.2979, -3.7903, -4.0637],\n",
      "        [-3.9034, -3.6385, -3.7349, -4.0829, -3.9484, -3.9342, -3.6550, -3.3545,\n",
      "         -3.8445, -3.9755, -3.5283, -3.6275, -3.7185, -4.3639, -3.8254, -3.7818,\n",
      "         -4.3031, -4.1578, -3.6504, -3.7936, -4.2949, -3.6770, -3.5003, -3.5579,\n",
      "         -3.8845, -3.7579, -3.5343, -3.7404, -3.6821, -4.3609, -3.9271, -3.6118,\n",
      "         -3.3645, -3.3885, -3.8800, -4.1137, -4.0594, -4.1999, -3.6329, -4.1710,\n",
      "         -4.0113, -3.6268, -4.3760, -3.7157, -4.0381],\n",
      "        [-3.8133, -3.6088, -3.7730, -4.0471, -3.9216, -3.9372, -3.6019, -3.5025,\n",
      "         -3.9324, -4.1313, -3.7198, -3.6612, -3.7155, -4.2658, -3.7940, -3.8582,\n",
      "         -4.3357, -4.2772, -3.7131, -3.8066, -4.2264, -3.5971, -3.4170, -3.4831,\n",
      "         -3.6232, -3.8058, -3.5270, -3.5745, -3.6088, -4.2153, -4.0495, -3.4424,\n",
      "         -3.4969, -3.5607, -3.9302, -4.1441, -4.1390, -4.0884, -3.6037, -4.2727,\n",
      "         -3.9364, -3.6629, -4.2707, -3.8066, -3.9186]])\n",
      "tensor([[ -0.1201,  -8.6865,  -6.6693,  -8.7672,  -7.7432, -14.6230, -12.3694,\n",
      "          -2.2916, -11.7418,  -8.4994,  -6.4960,  -9.2021, -18.5870,  -9.9516,\n",
      "         -11.0376, -13.2063, -12.0838,  -9.1633,  -9.5265, -11.1272,  -7.3137,\n",
      "          -9.7816, -10.2635,  -5.6655, -11.6726,  -8.4513, -12.9090, -10.1799,\n",
      "          -9.5626,  -7.9031,  -7.9655, -12.9179,  -9.4752, -10.9667, -10.0580,\n",
      "         -14.2836, -10.2729,  -9.0519,  -6.5120,  -7.2479, -14.9992,  -9.8621,\n",
      "          -9.4513, -12.2893, -11.8330],\n",
      "        [ -7.4815,  -4.5228,  -2.5724,  -1.2508, -10.2894,  -3.9320,  -8.8952,\n",
      "          -7.2186, -10.4836,  -5.9666,  -1.5711,  -5.7033,  -8.5479,  -7.0229,\n",
      "         -11.8542,  -5.2901, -10.7938,  -8.7875,  -8.7652,  -9.1068,  -1.4534,\n",
      "          -8.7072,  -3.9070,  -7.9949, -10.9197, -12.5180,  -9.3716,  -3.6045,\n",
      "          -8.7536,  -2.7829,  -5.5998, -12.0398,  -7.7341,  -6.5161,  -7.8606,\n",
      "          -7.4853,  -9.2844,  -3.4910, -10.2514,  -6.5344,  -7.5579,  -6.1089,\n",
      "         -13.0453, -10.0613, -10.4878],\n",
      "        [ -5.6216,  -0.4356,  -2.0649,  -4.1313,  -5.3726,  -2.0949,  -7.2869,\n",
      "          -7.2606, -15.0721, -12.7539,  -6.4692,  -3.1342, -13.2877,  -6.5395,\n",
      "          -8.9893,  -9.2048, -16.2434,  -9.6942,  -7.9432, -12.0187,  -6.0630,\n",
      "         -12.0748,  -6.9927,  -8.9117, -10.9196, -12.0520, -10.4254,  -7.3752,\n",
      "          -9.8605,  -4.9158,  -4.8624, -15.5393,  -9.2161,  -8.6371,  -6.9429,\n",
      "         -10.6873, -12.6798,  -6.0247,  -7.0386,  -8.0410,  -9.1688,  -5.0998,\n",
      "         -12.6018, -11.4238, -14.8965],\n",
      "        [ -7.6680,  -4.8617,  -2.6509,  -1.2454, -10.5632,  -3.7679,  -9.1731,\n",
      "          -7.5638, -10.9994,  -6.4265,  -1.5902,  -5.7792,  -8.9560,  -7.1991,\n",
      "         -12.4262,  -5.5918, -11.3438,  -8.9581,  -9.1954,  -9.3563,  -1.3391,\n",
      "          -9.0131,  -4.0843,  -8.4260, -11.6769, -13.1453,  -9.6648,  -3.7909,\n",
      "          -8.9306,  -2.8727,  -5.6966, -12.5167,  -7.8933,  -6.7486,  -8.1703,\n",
      "          -7.7939,  -9.6202,  -3.5773, -10.5307,  -6.5221,  -7.9433,  -6.2202,\n",
      "         -13.7064, -10.3526, -11.0104],\n",
      "        [ -7.4676,  -4.5510,  -5.5929,  -6.1464,  -0.9667,  -5.6891,  -5.6220,\n",
      "          -6.6907, -10.9429, -15.9111,  -7.6462,  -2.0626,  -9.3571,  -3.6133,\n",
      "          -1.3814,  -8.6050, -12.2507,  -6.4913,  -6.0667,  -7.5306,  -5.7673,\n",
      "          -7.9903,  -7.8209,  -8.1922,  -9.3009,  -7.3081,  -7.3509,  -8.0880,\n",
      "         -10.1698,  -7.0810,  -2.4624, -10.2565,  -6.7463,  -9.4360,  -3.6624,\n",
      "          -7.6387,  -8.6078,  -7.5011,  -3.8925,  -8.0892,  -6.3032,  -3.2082,\n",
      "          -7.8426,  -7.6831, -10.8334]])\n",
      "torch.return_types.max(\n",
      "values=tensor([-0.1201, -1.2508, -0.4356, -1.2454, -0.9667]),\n",
      "indices=tensor([0, 3, 1, 3, 4]))\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    # {'1': 0, '2-4': 1, '2': 2, '2-3-3-1-5': 3, '2-3-3-5': 4}\n",
    "    print(tag_scores)\n",
    "    print(torch.max(tag_scores,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7682926829268293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# \n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for sentence, tags in training_data:\n",
    "    # Prepare the sentence for the model\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    \n",
    "    # Get predicted tag indices\n",
    "    predicted_tags = torch.argmax(model(sentence_in), dim=1)\n",
    "    \n",
    "    # Convert tensor to numpy array\n",
    "    predicted_tags = predicted_tags.numpy()\n",
    "    \n",
    "    # Convert ground truth tags to indices\n",
    "    true_tags = [tag_to_ix[tag] for tag in tags]\n",
    "    \n",
    "    # Calculate accuracy for this sentence\n",
    "    correct_predictions += sum(predicted_tag == true_tag for predicted_tag, true_tag in zip(predicted_tags, true_tags))\n",
    "    total_predictions += len(tags)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy:\", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 3]\n",
      "Filter\n",
      "Project-Sort\n",
      "Project-Sort\n"
     ]
    }
   ],
   "source": [
    "test_seq=\"AfB BC CE\".split()\n",
    "# test_seq=\"CB BE\".split()\n",
    "inputs = prepare_sequence(test_seq, word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "ans = torch.max(tag_scores,1)\n",
    "ans_list = ans.indices.numpy().tolist()\n",
    "print(ans_list)\n",
    "for item in ans_list:\n",
    "    temp_str = ix_to_tag[item]\n",
    "    temp_list = temp_str.split(\"-\")\n",
    "    temp_ans = []\n",
    "    for key in temp_list:\n",
    "        temp_ans.append(id_to_operators[key])\n",
    "    print(\"-\".join(temp_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
